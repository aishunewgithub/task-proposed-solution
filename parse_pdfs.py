# -*- coding: utf-8 -*-
"""parse_pdfs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13JJTdjwSi2luJYGZ29dQzInPjD1jpeRH

Importing Necessary Libraries for the Project
"""

'''!pip install pdfplumber

!apt-get update
!apt-get install -y ghostscript libgl1
!pip install "camelot-py[cv]"

!pip install pytesseract

! pip install pdf2image

! pip install fitz

!pip install pymupdf

! pip install tools

!pip install pdfminer.high_level

#uploading csv file to Google colab and checking if uploaded
#from google.colab import files
#excel_sheet_file_url = files.upload()
#list(excel_sheet_file_url.keys())'''


"""Importing Necessary Libraries for the Project"""

#from google.colab import files
import pandas as pd
import numpy as np
import pdfplumber
import os
from io import BytesIO
import camelot
import datetime
import json
import pytesseract
from pdf2image import convert_from_bytes
import re
import requests
import fitz
import camelot
from pdfminer.high_level import extract_text as pdfminer_extract


#Reading the file in pandas df
excel_file_links_df = pd.read_csv(r"C:\Users\Lenovo\Downloads\Bilby-Interview-Challenge\Submission-PDF-Parser\Documents.csv", header=0)


"""# **STEP - 1**
#Function to check if OCR is required for the usecase or not
"""

'''

This Function checks if our uscase requires OCR implementation or not

'''
import pymupdf as fitz

#Checking text
def is_meaningful_text(s):

    # Allow Chinese, English letters, numbers, punctuation
    # If less than 30% of characters match this, text is garbage
    allowed = re.findall(r'[A-Za-z0-9\u4e00-\u9fff.,;:!?%()\-]', s)
    return len(allowed) / max(len(s), 1) > 0.3


#A simple function to check if A PDF have images which will need OCR for text detection
def pdf_needs_ocr_from_url(detect_url):

    # Download PDF
    pdf_bytes = requests.get(detect_url).content
    doc = fitz.open(stream=pdf_bytes, filetype="pdf")

    text_found = False
    meaningful_found = False
    image_heavy_pages = 0

    for page in doc:
        text = page.get_text("text").strip()

        if text:
            text_found = True
            if is_meaningful_text(text):
                meaningful_found = True

        # detect image-based page
        images = page.get_images(full=True)
        if len(images) > 0:
            image_heavy_pages += 1

    # Decision logic
    if not text_found:
        return True  # definitely needs OCR

    if text_found and not meaningful_found:
        return True  # garbage text → needs OCR

    if image_heavy_pages == len(doc):
        return True  # fully image based PDF

    return False  # text is meaningful → OCR optional

# ---- Test ----
#A sample document to check
detect_url = "https://storage.googleapis.com/bilby-scraper-pdfs-prod/China%2Fbank%2FPeopleBankOfChina2%2Fhttp%253A%252F%252Fwww.pbc.gov.cn%252Fdiaochatongjisi%252F116219%252F116227%252F5629762%252F2025032117120920108.pdf"

print("OCR needed:", pdf_needs_ocr_from_url(detect_url))

"""# **STEP - 2**
#Reading PDF's from provided URLS
"""

#Fetching URL links from excel sheet
def fetch_pdf(url):
    """
    Fetches a PDF from a URL and returns it as a BytesIO object.
    """
    try:
        response = requests.get(url, timeout=30)
        response.raise_for_status()  # Raises an error for bad status
        return BytesIO(response.content)
    except Exception as e:
        print(f"Error fetching {url}: {e}")
        return None


#bulk files
pdfs = []  # store (url, pdf_bytes)

#Iterating over URLS stored in Dataframe column - document_urls
for cell in excel_file_links_df['document_urls']:
    # Remove brackets and whitespace as Url format is [actual url]
    clean_cell = cell.strip().strip('[]')

    # Split by comma if multiple URLs are in single cell
    urls = [u.strip() for u in clean_cell.split(',')]

    #Creating array of urls
    for url in urls:
        pdf_bytes = fetch_pdf(url)
        if pdf_bytes is not None:
            pdfs.append((url, pdf_bytes))

count =0
for i in pdfs:
  count +=1
  #print(pdf_bytes)

print(count)

"""# **STEP - 3**
#Combined Parser

NOTE :- The exact format for JSON fields were not specified so while extracting data the generic assumptions have been considered
"""

#This function classifies documents, Extracts content and creats the expected JSON

#Excpected JSON

#Bulk Deal Report:
#{
#"doc_type": "bulk_deal",
#"source_url": "<https://example.com/sse_bulk_deals.pdf>",
#"extraction_timestamp": "2025-01-15T10:30:00Z",
#"metadata": {
#"report_date": "2025-09-20",
#"exchange": "SSE",
#"currency": "CNY",
#"language": "zh-CN"
#},
#"records": [
#{
#"security_name": "华发股份",
#"security_code": "600325",
#"client_name": "ABC Capital Management",
#"transaction_type": "BUY",
#"quantity": 1000000,

#"price": 25.50,
#"value": 25500000.0,
#"transaction_date": "2025-09-20"
#}
#],
#"data_quality": {
#"completeness_score": 0.95,
#"validation_warnings": []
#}
#}

#Board Meeting Announcement:
#{
#"doc_type": "board_meeting",
#"source_url": "<https://example.com/board_meeting.pdf>",
#"extraction_timestamp": "2025-01-15T10:30:00Z",
#"metadata": {
#"company": "Tellus Holdings",
#"company_code": "002345",
#"announcement_date": "2025-09-20",
#"exchange": "SZSE",
#"language": "zh-CN"
#},
#"details": {
#"meeting_date": "2025-10-05",
#"meeting_type": "Board Meeting",
#"purpose": "Consideration of quarterly results and dividend declaration",
#"resolutions": [
#{
#"resolution_number": "2025-001",
#"description": "Approval of Q3 financial statements",
#"status": "Approved"
#}
#]
#},
#"data_quality": {
#"completeness_score": 0.88,
#"validation_warnings": ["Meeting venue not specified"]
#}
#}


#A single Parser

#Converts into safe float
# Example: "1,234.56" → "1234.56"
def safe_float(x):
    try:
        return float(str(x).replace(",", "").strip())
    except:
        return None

#Extracting Date
def extract_date(text):
    m = re.search(r"(20\d{2}[-/年]\d{1,2}[-/月]\d{1,2})", text)
    return m.group(0) if m else None


#  text extraction and fallback OCR
def extract_text(pdf_bytes):
    try:
        with pdfplumber.open(pdf_bytes) as pdf:
            text = "\n".join([page.extract_text() or "" for page in pdf.pages])
        if text.strip():
            return text
    except:
        pass

    # OCR fallback
    try:
        images = convert_from_bytes(pdf_bytes.getvalue(), dpi=300)
        text = ""
        for img in images:
            text += pytesseract.image_to_string(img, lang="chi_sim") + "\n"
        return text
    except:
        return ""


# TABLE EXTRACTION (Camelot → pdfplumber fallback)
def extract_tables(pdf_bytes):
    tables = []

    # Camelot
    try:
        with open("/tmp/tmp.pdf", "wb") as f:
            f.write(pdf_bytes.getbuffer())

        try:
            cam_tables = camelot.read_pdf("/tmp/tmp.pdf", pages="all", flavor="lattice")
            for tb in cam_tables:
                df = tb.df.replace("", np.nan).infer_objects()
                tables.append(df)
        except:
            pass
    except:
        pass

    # pdfplumber fallback
    if len(tables) == 0:
        try:
            with pdfplumber.open(pdf_bytes) as pdf:
                for page in pdf.pages:
                    table = page.extract_table()
                    if table:
                        tables.append(pd.DataFrame(table))
        except:
            pass

    return tables


# BOARD MEETING EXTRACTION
def extract_board_meeting_info(text: str) -> dict:
    data = {}

    date_patterns = [
        r"(\d{4}年\d{1,2}月\d{1,2}日)",
        r"(\d{4}-\d{1,2}-\d{1,2})",
        r"(\d{4}/\d{1,2}/\d{1,2})",
    ]
    for p in date_patterns:
        m = re.search(p, text)
        if m:
            data["meeting_date"] = m.group(1)
            break

    meeting_type_keywords = {
        "board_meeting": ["董事会会议", "董事会议", "Board Meeting", "董事会决议"],
        "shareholder_meeting": ["股东大会", "年度股东大会"],
    }
    for key, words in meeting_type_keywords.items():
        for w in words:
            if w in text:
                data["meeting_type"] = key
                break

    purpose_patterns = [
        r"(关于.*?的公告)",
        r"(审议.*?事项)",
        r"(会议目的[:：]\s*.*)",
    ]
    for p in purpose_patterns:
        m = re.search(p, text)
        if m:
            data["purpose"] = m.group(1)
            break

    res_pattern = r"(议案|决议).*?(通过|批准|审议)"
    resolutions = re.findall(res_pattern, text, flags=re.DOTALL)
    if resolutions:
        data["resolutions"] = [
            {"description": r[0] + r[1], "status": "Approved"} for r in resolutions
        ]

    return data


# BULK DEAL EXTRACTION (fallback regex)

def extract_bulk_deal_info(text: str) -> list:
    records = []

    name_pattern = r"([A-Za-z0-9\u4e00-\u9fa5]{2,20}(股份|有限公司|集团))"
    code_pattern = r"\b(60\d{4}|000\d{3}|30\d{4})\b"
    type_pattern = r"(买入|卖出|增持|减持|BUY|SELL)"
    num_pattern = r"(\d{1,3}(,\d{3})*(\.\d+)?|\d+\.\d+|\d+)"
    date_pattern = r"(\d{4}[年/-]\d{1,2}[月/-]\d{1,2}[日]?)"

    for para in text.split("\n"):
        record = {}

        if name := re.search(name_pattern, para):
            record["security_name"] = name.group(1)

        if code := re.search(code_pattern, para):
            record["security_code"] = code.group(1)

        if ttype := re.search(type_pattern, para):
            record["transaction_type"] = ttype.group(1)

        quantity = re.search(num_pattern, para)
        if quantity:
            record["quantity"] = safe_float(quantity.group(1))

        nums = re.findall(num_pattern, para)
        if len(nums) >= 2:
            record["price"] = safe_float(nums[1][0])

        if date := re.search(date_pattern, para):
            record["transaction_date"] = date.group(1)

        if len(record.keys()) >= 2:
            records.append(record)

    return records




# ENFORCE JSON ORDER

def enforce_json_order(obj):
    ordered = {}

    ordered_fields = [
        "doc_type",
        "source_url",
        "extraction_timestamp",
        "metadata",
        "records",
        "details"
    ]

    for key in ordered_fields:
        if key in obj:
            ordered[key] = obj[key]

    if "data_quality" in obj:
        ordered["data_quality"] = obj["data_quality"]

    return ordered



# MASTER PARSER - combine Parser

def parse_document_complete(url, pdf_bytes):

    # 1) TEXT
    plumber_text = extract_text(pdf_bytes) or ""
    text = plumber_text

    # 2) TABLES
    tables = extract_tables(pdf_bytes)

    # 3) CLASSIFY
    doc_type = (
        "bulk_deal" if (len(tables) > 0 and len(text.strip()) < 20)
        else "board_meeting" if (len(text.strip()) > 20 and len(tables) == 0)
        else "hybrid" if (len(tables) > 0 and len(text.strip()) > 20)
        else "generic"
    )

    # 4) METADATA
    metadata = {
        "report_date": extract_date(text),
        "exchange": None,
        "currency": "CNY",
        "language": "zh-CN"
    }

    # 5) TABLE → RECORDS
    records = []

    for df in tables:
        if len(df) < 2:
            continue

        df.columns = df.iloc[0]
        df = df[1:].reset_index(drop=True)

        colmap = {
            "证券名称": "security_name",
            "证券代码": "security_code",
            "成交价格": "price",
            "价格": "price",
            "数量": "quantity",
            "成交数量": "quantity",
            "买方": "client_name",
            "卖方": "client_name",
        }
        df = df.rename(columns={c: colmap.get(c, c) for c in df.columns})

        for _, row in df.iterrows():
            qty = safe_float(row.get("quantity"))
            prc = safe_float(row.get("price"))

            rec = {
                "security_name": row.get("security_name"),
                "security_code": row.get("security_code"),
                "client_name": row.get("client_name"),
                "transaction_type": "BUY" if "买" in str(row.get("client_name", "")) else None,
                "quantity": qty,
                "price": prc,
                "value": qty * prc if qty and prc else None,
                "transaction_date": extract_date(text)
            }

            if rec["security_name"] or rec["security_code"]:
                records.append(rec)

    # fallback regexp
    if len(records) == 0:
        for r in extract_bulk_deal_info(text):
            qty = safe_float(r.get("quantity"))
            prc = safe_float(r.get("price"))

            records.append({
                "security_name": r.get("security_name"),
                "security_code": r.get("security_code"),
                "client_name": None,
                "transaction_type": r.get("transaction_type"),
                "quantity": qty,
                "price": prc,
                "value": qty * prc if qty and prc else None,
                "transaction_date": r.get("transaction_date")
            })

    # 6) BOARD MEETING DETAILS
    details = None
    meeting_info = extract_board_meeting_info(text)
    if meeting_info:
        details = meeting_info

    # 7) DATA QUALITY
    has_text = len(text.strip()) > 0
    has_tables = len(tables) > 0

    data_quality = {
        "completeness_score": 1.0 if (has_text or has_tables) else 0.0,
        "validation_warnings": [] if (has_text or has_tables) else ["No extractable content found"]
    }

    # 8) FINAL JSON
    timestamp = datetime.datetime.now(datetime.timezone.utc).isoformat()

    # Decide primary: records OR details
    if records:
        key_name = "records"
        key_value = records
    else:
        key_name = "details"
        key_value = details

    raw = {
        "doc_type": doc_type,
        "source_url": url,
        "extraction_timestamp": timestamp,
        "metadata": metadata,
        key_name: key_value,
        "data_quality": data_quality
    }

    return enforce_json_order(raw)



"""Main Entry Point"""

# MAIN ENTRY POINT

def main():
    # 1. Load CSV
    excel_file = "Documents.csv"   # Change if needed
    df = pd.read_csv(excel_file)


    # 2. Extract URLs
    print("Extracting PDFs from provided URLS")
    pdfs = []
    for cell in df['document_urls']:
        clean_cell = cell.strip().strip('[]')
        urls = [u.strip() for u in clean_cell.split(',')]
        for url in urls:
            pdf_bytes = fetch_pdf(url)
            if pdf_bytes:
                pdfs.append((url, pdf_bytes))

    # 3. Run parser on first N PDFs
    #N = 10  # LIMIT HOW MANY FILES YOU PARSE
    parsed_results = []

    print("Processing started for PDF's")
    for url, pdf_bytes in pdfs:
        print(f"Processing: {url}")
        result = parse_document_complete(url, pdf_bytes)
        parsed_results.append(result)

    # 4. Save output JSON
    with open("parsed_results.json", "w", encoding="utf-8") as f:
        json.dump(parsed_results, f, ensure_ascii=False, indent=2)

    print("\n✔ Finished! Output saved to parsed_results.json")

    print("Sample Output Result:\n", json.dumps(parsed_results[0], ensure_ascii=False, indent=2))


# RUN ONLY IF FILE IS EXECUTED DIRECTLY

if __name__ == "__main__":
    main()